{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo definitivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHONA ME DIJO QUE USARA LOS DATOS SIN PCA Y QUE HAGA UN POOLING GRANDE AL PRINCIPIO DE LA CNN PARA ACHICAR LOS DATOS DE UNA CUANDO ENTRAN. \n",
    "ADEMÁS, TENGO QUE HACER CONV2D EN VES DE 1D Y TENGO QUE AGREGARLE LOS NUEVOS DATOS. ADEMÁS TENGO QUE USAR EL PRETRAINED MODEL RESNET QUE USO \n",
    "EL INDIO EN EL VIDEO PARA TENER YO YA HECHO EL COSO Y CON LOS WEIGHTS Y QUE NO ME TRADE TANTO EL ENTRENAMIENTO. YO LE CAMBIARÍA LO DEL POOL AL \n",
    "PRINCIPIO Y LO ADAPTARÍA AL FINAL DE LA RED PARA QUE SE AJUSTE A MIS DATOS.\n",
    "- TENGO QUE CORRER EL CODIGO PARA EL DATASET DE NUEVO PERO SIN EL PCA \n",
    "- VOLVER A ARREGLAR LA CNN PARA QUE ADMITA LA VIEJA INPUT SHAPE\n",
    "- USAR UN MODELO RESNET PREENTRENADO CON DATOS COHERENTES (PREFERENTEMENTE EL QUE USÓ EL INDIO)\n",
    "- INCORPORAR LO QUE ME DIJO CHONA DE CONV2D Y UN POOLING PARA QUE SE ADAPTE A RESNET (AdaptiveAvgPool2d - https://stackoverflow.com/questions/61955991/change-input-shape-dimensions-for-resnet-model-pytorch)\n",
    "\n",
    "CODIGOS DEL INDIO:\n",
    "https://github.com/talhaanwarch/youtube-tutorials/blob/main/eeg-conv2d.ipynb\n",
    "https://github.com/talhaanwarch/youtube-tutorials/blob/main/eeg_epilepsy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, GlobalAveragePooling2D, MaxPooling2D, Dropout, Input\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "#import tensorflowjs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('C:\\\\Users\\\\47575909\\\\Desktop\\\\X_12.npy')\n",
    "y = np.load('C:\\\\Users\\\\47575909\\\\Desktop\\\\y_12.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1790, 224, 224, 3), (1790,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.reshape(1790, 244, 244, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X[:,:224,:224,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1790, 224, 224, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "\n",
    "#Cargo el modelo ResNet50 preentrenado\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "\n",
    "#Congelo las capas del modelo base para que no se actualicen en el entrenamiento\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Agrego GlobalAveragePooling2D para adaptar la salida del modelo base\n",
    "#x = GlobalAveragePooling2D()(base_model.output)\n",
    "\n",
    "#Agrego capas adicionales para la clasificación de mis datos\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(500, activation='relu')(x)  # Puedes usar Dense en lugar de Conv2D aquí\n",
    "x = Dense(200, activation='relu')(x)  # Capas Dense en lugar de MaxPooling2D\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "output = Dense(1, activation='softmax')(x)\n",
    "\n",
    "'''#x = base_model.output\n",
    "#x = GlobalAveragePooling2D()(x)\n",
    "x = Conv2D(filters=16, kernel_size=3, activation='relu')(base_model.output)\n",
    "x = MaxPooling2D(pool_size=2)(x)\n",
    "x = Conv2D(filters=16, kernel_size=3, activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(8, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4, activation='relu')(x)\n",
    "\n",
    "output = Dense(1, activation='softmax')(x)'''\n",
    "\n",
    "#Junto todo y creo el modelo\n",
    "model = Model(inputs=input_tensor, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPILO\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEIGHTS AND BIASES PARA VER EL PROGRESO DESDE CASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "44/56 [======================>.......] - ETA: 13s - loss: 0.0296 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(archivo_x, archivo_y):\n",
    "    X = np.load(archivo_x)\n",
    "    y = np.load(archivo_y)\n",
    "\n",
    "    #DIVIDO EN TRAIN, TEST Y EVALUATION\n",
    "    #Divido los datos de train de los de test y evaluation (70/30)\n",
    "    X_train, X_testVal, y_train, y_testVal = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    #Divido los datos de test de los de evaluation (50/50 del 30% anterior)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_testVal, y_testVal, test_size=0.5, random_state=42)\n",
    "\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "    #EVALÚO\n",
    "    test_loss, test_acc = model.evaluate(X_val, y_val)\n",
    "\n",
    "    #VEO LA PERFORMANCE\n",
    "    print(\"Accuracy : \", test_acc)\n",
    "    print(\"Loss : \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [8181, 2727]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m archivo_x \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directorio, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m archivo_y \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directorio, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchivo_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchivo_y\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m, in \u001b[0;36mmodel_train\u001b[1;34m(archivo_x, archivo_y)\u001b[0m\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(archivo_y)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#DIVIDO EN TRAIN, TEST Y EVALUATION\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#Divido los datos de train de los de test y evaluation (70/30)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m X_train, X_testVal, y_train, y_testVal \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#Divido los datos de test de los de evaluation (50/50 del 30% anterior)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m X_val, X_test, y_val, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_testVal, y_testVal, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2646\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2646\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2648\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2649\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2650\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2651\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:453\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    452\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 453\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [8181, 2727]"
     ]
    }
   ],
   "source": [
    "directorio = 'C:\\\\Users\\\\47575909\\\\Desktop\\\\Dataset'\n",
    "num_archivos = 10\n",
    "\n",
    "for i in range(1, num_archivos + 1):\n",
    "    archivo_x = os.path.join(directorio, f'X_{i}.npy')\n",
    "    archivo_y = os.path.join(directorio, f'y_{i}.npy')\n",
    "\n",
    "    model_train(archivo_x, archivo_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''X = np.load('C:\\\\Users\\\\47575909\\\\Desktop\\\\X.npy')\n",
    "y = np.load('C:\\\\Users\\\\47575909\\\\Desktop\\\\y.npy')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#DIVIDO EN TRAIN, TEST Y EVALUATION\n",
    "#Divido los datos de train de los de test y evaluation (70/30)\n",
    "X_train, X_testVal, y_train, y_testVal = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#Divido los datos de test de los de evaluation (50/50 del 30% anterior)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_testVal, y_testVal, test_size=0.5, random_state=42)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#ENTRENO\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "#EVALÚO\n",
    "test_loss, test_acc = model.evaluate(X_val, y_val)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#VEO LA PERFORMANCE\n",
    "print(\"Accuracy : \", test_acc)\n",
    "print(\"Loss : \", test_loss)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HAGO UNA PREDICCIÓN\n",
    "prediction = model.predict(datos_test)\n",
    "epilepsy_prob = predicciones[:, 1]\n",
    "print(\"Probabilidad de tener epilepsia:\", epilepsy_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decirle a chona que el indio usa ResNet-26D y que keras no lo tiene ese. Que hago?...\n",
    "- Uso otro modelo de ResNet\n",
    "- Uso las liberías pytorch y timm que si tienen el ResNet-26D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportamos el modelo\n",
    "(Con tensorflow.js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportamos el modelooo\n",
    "tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model --signature_name=serving_default --saved_model_tags=serve /ruta/al/modelo/saved_model /ruta/de/destino/\n",
    "#che, no entiendo lo de las rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "//HABRÍA QUE HACER ESTO PARA QUE ME DEVUELVA EN LA WEB SOLO LA PROBABILIDAD QUE YO QUIERO (la probabilidad de tener epilepsia)\n",
    "\n",
    "/*// Obtener datos de entrada (pueden ser simulados en este ejemplo)\n",
    "const datosEntrada = obtenerDatosDeEntrada();\n",
    "\n",
    "// Cargar el modelo\n",
    "const modelo = await cargarModelo();\n",
    "\n",
    "// Realizar predicciones\n",
    "const predicciones = modelo.predict(tf.tensor(datosEntrada));\n",
    "\n",
    "// Mostrar la probabilidad de tener epilepsia\n",
    "const probabilidadEpilepsia = predicciones.dataSync()[1];\n",
    "console.log('Probabilidad de tener epilepsia:', probabilidadEpilepsia);*/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
