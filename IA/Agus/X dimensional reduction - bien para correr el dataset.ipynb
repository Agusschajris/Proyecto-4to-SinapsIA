{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa5b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import ssqueezepy #para convertir la señal en imagen\n",
    "from ssqueezepy import cwt\n",
    "from ssqueezepy.visuals import plot, imshow\n",
    "import os\n",
    "import mne #eeg analysis library\n",
    "import scipy.io\n",
    "#import torch.nn as nn\n",
    "#import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "934decab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2ddce-784d-45ef-8307-e6953ba8f8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "355c3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "#PRUEBO CON MENOS DATOS\n",
    "#ACA SUBO LOS ARCHIVOS DE TFRECORD\n",
    "TFrecord_directory = 'C:\\\\Users\\\\47575909\\\\Desktop\\\\tfrecord_data' \n",
    "\n",
    "#lista de archivos TFRecord en la carpeta\n",
    "filenames = [os.path.join(TFrecord_directory, f) for f in os.listdir(TFrecord_directory) if f.endswith('.tfrecord')]\n",
    "\n",
    "#Hago un dataset de TensorFlow a partir de los archivos TFRecord\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "#función para parsear los datos de TFRecord\n",
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        'grpno': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'path': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    return example\n",
    "\n",
    "parsed_dataset = dataset.map(parse_tfrecord_fn)\n",
    "\n",
    "#batching para argar los datos en lotes\n",
    "batch_size = 5  #puedo cambiar el tamaño (en la PC del colegio el máximo es 16, pero capáz que le tengo que poner menos)\n",
    "batched_dataset = parsed_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecda7582",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'concatenate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-23362d3a63b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m#agrego los scaleograms y labels a las listas X y y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'concatenate'"
     ]
    }
   ],
   "source": [
    "'''#SEPARO EN X E Y\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "#itero los lotes\n",
    "for batch_data in batched_dataset:\n",
    "    paths = batch_data['path'].numpy()\n",
    "    labels = batch_data['label'].numpy()\n",
    "    \n",
    "    #cargo los scaleograms desde los archivos .npy\n",
    "    batch_X = []\n",
    "    for path in paths:\n",
    "        spectrogram = np.load(path.decode('utf-8'))\n",
    "        batch_X.append(spectrogram)\n",
    "    \n",
    "#//////NUEVO\n",
    "\n",
    "    #primero tengo que convertirlo en un archivo numpy porque sino no quiere\n",
    "    batch_X = np.array(batch_X)\n",
    "    \n",
    "    X_shape = batch_X.shape\n",
    "    shape_0 = X_shape[0]\n",
    "\n",
    "    # Aplanar\n",
    "    batch_X = batch_X.reshape(shape_0, -1)\n",
    "    \n",
    "    #NORMALIZO X en rango [-1, 1] para que el cero quede en el centro\n",
    "    min_val = np.min(batch_X)\n",
    "    max_val = np.max(batch_X)\n",
    "\n",
    "    X = -1 + 2 * (batch_X - min_val) / (max_val - min_val)\n",
    "    \n",
    "    variance_to_keep = 0.95  #me quedo con el 95% de la varianza, esto es flexible (mientras más porcentaje, mayor va  quedar la dimensionalidad de mi X)\n",
    "\n",
    "    pca = PCA()\n",
    "    batch_X = pca.fit_transform(batch_X)  #aplico pca para los datos escalados\n",
    "\n",
    "    #Encuentro el número mínimo de componentes principales para alcanzar la varianza deseada\n",
    "    cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "    #('n_components_to_keep' contendrá el número mínimo de componentes principales necesarios para explicar al menos el 95% de la varianza en los datos.)\n",
    "    n_components_to_keep = np.argmax(cumulative_variance_ratio >= variance_to_keep) + 1 #chona me dijo que me deberían quedar por ejemplo 1000 atributos\n",
    "\n",
    "    #Selecciono la cant de componentes principales que me salió\n",
    "    batch_X = batch_X[:, :n_components_to_keep]\n",
    "    \n",
    "#//////NUEVO\n",
    "    \n",
    "    #agrego los scaleograms y labels a las listas X y y\n",
    "    X.concatenate(batch_X)\n",
    "    y.extend(labels)\n",
    "\n",
    "\n",
    "#convierto X e y a formato .npy\n",
    "X = np.concatenate(X, axis=0)\n",
    "y = np.array(y)\n",
    "\n",
    "print('Ya se separó todo!')\n",
    "print('Salió todo bien, tranquina :))')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b294652f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "190it [00:07, 25.98it/s]                                                                                               "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m variance_to_keep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m  \u001b[38;5;66;03m# Me quedo con el 95% de la varianza\u001b[39;00m\n\u001b[0;32m     36\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA()\n\u001b[1;32m---> 37\u001b[0m batch_X \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Aplica PCA para los datos escalados\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Asegúrate de que todos los lotes tengan la misma cantidad de componentes principales\u001b[39;00m\n\u001b[0;32m     40\u001b[0m n_components_to_keep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_components, batch_X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:460\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    439\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 460\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m     U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:510\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:536\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[1;34m(self, X, n_components)\u001b[0m\n\u001b[0;32m    534\u001b[0m U, S, Vt \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39msvd(X, full_matrices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# flip eigenvectors' sign to enforce deterministic output\u001b[39;00m\n\u001b[1;32m--> 536\u001b[0m U, Vt \u001b[38;5;241m=\u001b[39m \u001b[43msvd_flip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m components_ \u001b[38;5;241m=\u001b[39m Vt\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# Get variance explained by singular values\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:769\u001b[0m, in \u001b[0;36msvd_flip\u001b[1;34m(u, v, u_based_decision)\u001b[0m\n\u001b[0;32m    764\u001b[0m         out[:, n] \u001b[38;5;241m=\u001b[39m arrays[n][ix[:, n]]\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m--> 769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msvd_flip\u001b[39m(u, v, u_based_decision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    770\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sign correction to ensure deterministic output from SVD.\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \n\u001b[0;32m    772\u001b[0m \u001b[38;5;124;03m    Adjusts the columns of u and the rows of v such that the loadings in the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;124;03m        Array v with adjusted rows and the same dimensions as v.\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m u_based_decision:\n\u001b[0;32m    803\u001b[0m         \u001b[38;5;66;03m# columns of u, rows of v\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "190it [00:20, 25.98it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define el número máximo de componentes principales que deseas mantener para todos los lotes\n",
    "max_components = 1000  # Puedes ajustar este valor según tus necesidades\n",
    "\n",
    "X_list = []  # Lista temporal para X\n",
    "y_list = []  # Lista temporal para y\n",
    "\n",
    "# Crea un objeto tqdm para rastrear el progreso\n",
    "pbar = tqdm(total=len(filenames))\n",
    "\n",
    "# Itera los lotes\n",
    "for batch_data in batched_dataset:\n",
    "    paths = batch_data['path'].numpy()\n",
    "    labels = batch_data['label'].numpy()\n",
    "    \n",
    "    # Cargar los scaleograms desde los archivos .npy\n",
    "    batch_X = []\n",
    "    for path in paths:\n",
    "        spectrogram = np.load(path.decode('utf-8'))\n",
    "        batch_X.append(spectrogram)\n",
    "    \n",
    "    # Convierte batch_X en un arreglo NumPy\n",
    "    batch_X = np.array(batch_X)\n",
    "    \n",
    "    batch_X = batch_X.reshape((batch_X.shape[0], -1))  # Aplanar los datos\n",
    "    \n",
    "    # NORMALIZA X en rango [-1, 1] para que el cero quede en el centro\n",
    "    min_val = np.min(batch_X)\n",
    "    max_val = np.max(batch_X)\n",
    "\n",
    "    batch_X = -1 + 2 * (batch_X - min_val) / (max_val - min_val)\n",
    "    \n",
    "    variance_to_keep = 0.95  # Me quedo con el 95% de la varianza\n",
    "\n",
    "    pca = PCA()\n",
    "    batch_X = pca.fit_transform(batch_X)  # Aplica PCA para los datos escalados\n",
    "\n",
    "    # Asegúrate de que todos los lotes tengan la misma cantidad de componentes principales\n",
    "    n_components_to_keep = min(max_components, batch_X.shape[1])\n",
    "    if batch_X.shape[1] < max_components:\n",
    "        # Rellena con ceros las columnas faltantes si es necesario\n",
    "        zeros_to_add = max_components - batch_X.shape[1]\n",
    "        zeros = np.zeros((batch_X.shape[0], zeros_to_add))\n",
    "        batch_X = np.hstack((batch_X, zeros))\n",
    "    else:\n",
    "        batch_X = batch_X[:, :n_components_to_keep]\n",
    "\n",
    "    # Agrega los scaleograms y labels a X_list e y_list\n",
    "    X_list.append(batch_X)\n",
    "    y_list.extend(labels)\n",
    "\n",
    "    # Actualiza la barra de progreso\n",
    "    pbar.update(len(paths))  # Actualiza la barra de progreso según la cantidad de archivos en el lote\n",
    "\n",
    "# Convierte X_list e y_list en arreglos NumPy\n",
    "X = np.concatenate(X_list, axis=0)\n",
    "y = np.array(y_list)\n",
    "\n",
    "# Cierra la barra de progreso\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''X_shape = X.shape\n",
    "shape_0 = X_shape[0]\n",
    "X_shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f01d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplanar tus datos\n",
    "X = X.reshape(shape_0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e04607dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZO X en rango [-1, 1] para que el cero quede en el centro\n",
    "min_val = np.min(X)\n",
    "max_val = np.max(X)\n",
    "\n",
    "X = -1 + 2 * (X - min_val) / (max_val - min_val)\n",
    "\n",
    "#X = (X - X.min()) / (X.max() - X.min()) [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b37cbb17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bc5f1a0adac8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49216ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "variance_to_keep = 0.95  #me quedo con el 95% de la varianza, esto es flexible (mientras más porcentaje, mayor va  quedar la dimensionalidad de mi X)\n",
    "\n",
    "pca = PCA()\n",
    "X = pca.fit_transform(X)  #aplico pca para los datos escalados\n",
    "\n",
    "#Encuentro el número mínimo de componentes principales para alcanzar la varianza deseada\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "#('n_components_to_keep' contendrá el número mínimo de componentes principales necesarios para explicar al menos el 95% de la varianza en los datos.)\n",
    "n_components_to_keep = np.argmax(cumulative_variance_ratio >= variance_to_keep) + 1 #chona me dijo que me deberían quedar por ejemplo 1000 atributos\n",
    "\n",
    "#Selecciono la cant de componentes principales que me salió\n",
    "X = X[:, :n_components_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Me quedé con \" + str(n_components_to_keep) + \" componentes principales\")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ceaef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Varianza explicada por cada componente principal:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afd8d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOS GUARDO EN MI COMPUTADORA\n",
    "#np.save('X.npy', X) está guardada normalizada en rango [0, 1], habría que cambiar eso\n",
    "np.save('X_PCA.npy', X) #esto es para probar\n",
    "np.save('y_PCA.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no se si lo que hice está bien, chona me dijo que usara PaCMAP pero no lo entiendo, y acá encontre que con sklearn medio que se puede hacer asi que :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d392e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hago una función?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
